name: Test Suite

on:
  push:
    branches: [main, develop]
    paths: 
      - 'scripts/**'
      - 'tests/**'
      - 'requirements.txt'
      - 'pytest.ini'
      - '.github/workflows/test.yml'
  pull_request:
    branches: [main, develop]
    paths:
      - 'scripts/**'
      - 'tests/**'
      - 'requirements.txt'
      - 'pytest.ini'
      - '.github/workflows/test.yml'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Setup test environment variables
        run: |
          echo "AWS_REGION=us-east-1" >> $GITHUB_ENV
          echo "AWS_S3_BUCKET=test-bucket" >> $GITHUB_ENV
          echo "SPOTIFY_CLIENT_ID=test_client_id" >> $GITHUB_ENV
          echo "SPOTIFY_CLIENT_SECRET=test_client_secret" >> $GITHUB_ENV
          echo "SPOTIFY_REFRESH_TOKEN=test_refresh_token" >> $GITHUB_ENV
          echo "SPOTIFY_SHOW_ID=test_show_id" >> $GITHUB_ENV
          echo "BASE_URL=https://cdn.test.com" >> $GITHUB_ENV
          echo "PODCAST_TITLE=Test Podcast" >> $GITHUB_ENV
          echo "PODCAST_DESCRIPTION=Test podcast description" >> $GITHUB_ENV
          echo "PODCAST_AUTHOR=Test Author" >> $GITHUB_ENV
          echo "PODCAST_EMAIL=test@example.com" >> $GITHUB_ENV
          
      - name: Run unit tests
        run: |
          pytest tests/ \
            --verbose \
            --tb=short \
            --cov=scripts \
            --cov-report=term-missing \
            --cov-report=xml \
            --junit-xml=test-results.xml \
            -m "not slow and not integration and not network"
            
      - name: Run integration tests
        run: |
          pytest tests/ \
            --verbose \
            --tb=short \
            -m "integration and not network" \
            --junit-xml=integration-test-results.xml
            
      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results
          path: |
            test-results.xml
            integration-test-results.xml
            coverage.xml
            htmlcov/
          retention-days: 30
          
      - name: Report test summary
        uses: dorny/test-reporter@v1
        if: always()
        with:
          name: Test Results
          path: '*.xml'
          reporter: java-junit
          fail-on-error: true
          
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

  lint:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Run Black (code formatting check)
        run: |
          black --check --diff scripts/ tests/
          
      - name: Run Flake8 (linting)
        run: |
          flake8 scripts/ tests/ \
            --max-line-length=100 \
            --extend-ignore=E203,W503 \
            --exclude=__pycache__
            
      - name: Run MyPy (type checking)
        run: |
          mypy scripts/ \
            --ignore-missing-imports \
            --no-strict-optional \
            --warn-return-any \
            --warn-unused-configs
            
      - name: Check import sorting
        run: |
          python -c "
          import ast
          import sys
          from pathlib import Path
          
          def check_imports(file_path):
              with open(file_path) as f:
                  try:
                      tree = ast.parse(f.read())
                  except SyntaxError:
                      return True
              
              imports = []
              for node in ast.walk(tree):
                  if isinstance(node, (ast.Import, ast.ImportFrom)):
                      imports.append(node.lineno)
              
              # Check if imports are at the top (within first 20 lines)
              return not imports or max(imports) <= 20
          
          scripts_dir = Path('scripts')
          tests_dir = Path('tests')
          
          issues = []
          for py_file in list(scripts_dir.glob('*.py')) + list(tests_dir.glob('*.py')):
              if not check_imports(py_file):
                  issues.append(str(py_file))
          
          if issues:
              print('Import order issues found in:', ', '.join(issues))
              sys.exit(1)
          else:
              print('Import order check passed')
          "

  security:
    name: Security Checks
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety
          
      - name: Run Bandit (security linting)
        run: |
          bandit -r scripts/ \
            -f json \
            -o bandit-report.json \
            --severity-level medium \
            --confidence-level medium
            
      - name: Run Safety (dependency vulnerability check)
        run: |
          safety check \
            --json \
            --output safety-report.json \
            --continue-on-error
            
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
          retention-days: 30

  performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Setup test environment variables
        run: |
          echo "AWS_REGION=us-east-1" >> $GITHUB_ENV
          echo "AWS_S3_BUCKET=test-bucket" >> $GITHUB_ENV
          echo "BASE_URL=https://cdn.test.com" >> $GITHUB_ENV
          echo "PODCAST_TITLE=Performance Test Podcast" >> $GITHUB_ENV
          echo "PODCAST_DESCRIPTION=Performance test description" >> $GITHUB_ENV
          echo "PODCAST_AUTHOR=Performance Test Author" >> $GITHUB_ENV
          echo "PODCAST_EMAIL=performance@test.com" >> $GITHUB_ENV
          
      - name: Run performance tests
        run: |
          pytest tests/ \
            --verbose \
            --tb=short \
            -m "slow" \
            --durations=10 \
            --junit-xml=performance-test-results.xml
            
      - name: Upload performance test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: performance-test-results.xml
          retention-days: 30

  test-matrix:
    name: Test on Multiple Python Versions
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request' || github.event_name == 'workflow_dispatch'
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
      fail-fast: false
      
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Setup test environment variables
        run: |
          echo "AWS_REGION=us-east-1" >> $GITHUB_ENV
          echo "AWS_S3_BUCKET=test-bucket" >> $GITHUB_ENV
          echo "BASE_URL=https://cdn.test.com" >> $GITHUB_ENV
          echo "PODCAST_TITLE=Matrix Test Podcast" >> $GITHUB_ENV
          echo "PODCAST_DESCRIPTION=Matrix test description" >> $GITHUB_ENV
          echo "PODCAST_AUTHOR=Matrix Test Author" >> $GITHUB_ENV
          echo "PODCAST_EMAIL=matrix@test.com" >> $GITHUB_ENV
          
      - name: Run core tests
        run: |
          pytest tests/ \
            --verbose \
            -m "not slow and not integration and not network" \
            --tb=line \
            --junit-xml=matrix-test-results-${{ matrix.python-version }}.xml
            
      - name: Upload matrix test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: matrix-test-results-${{ matrix.python-version }}
          path: matrix-test-results-${{ matrix.python-version }}.xml
          retention-days: 7

  validate-scripts:
    name: Validate Script Functionality
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          
      - name: Validate script imports
        run: |
          echo "Validating all scripts can be imported..."
          for script in scripts/*.py; do
            echo "Checking $script"
            python -c "
          import sys
          import os
          sys.path.insert(0, 'scripts')
          script_name = '$(basename "$script" .py)'
          try:
              __import__(script_name)
              print(f'✅ {script_name} imports successfully')
          except Exception as e:
              print(f'❌ {script_name} import failed: {e}')
              sys.exit(1)
          "
          done
          
      - name: Validate script help messages
        run: |
          echo "Validating script help messages..."
          for script in scripts/*.py; do
            echo "Checking help for $script"
            python "$script" --help > /dev/null || {
              echo "❌ Help check failed for $script"
              exit 1
            }
            echo "✅ Help check passed for $script"
          done
          
      - name: Test RSS generation with sample data
        run: |
          echo "Testing RSS generation..."
          python scripts/build_rss.py \
            --bucket "test-bucket" \
            --base-url "https://cdn.test.com" \
            --commit-sha "test123" \
            --episode-metadata '{"slug":"20250618-test","title":"Test","description":"Test desc","pub_date":"2025-06-18T10:00:00+00:00","duration_seconds":1800,"file_size_bytes":25000000,"mp3_url":"https://cdn.test.com/test.mp3","guid":"test-guid","s3_key":"podcast/2025/test.mp3","year":2025}' \
            --help > /dev/null
          echo "✅ RSS generation script validated"
          
      - name: Test metadata validation
        run: |
          echo "Testing metadata validation..."
          python scripts/validate_metadata.py \
            --metadata '{"slug":"20250618-test","title":"Test","description":"Test desc","pub_date":"2025-06-18T10:00:00+00:00","duration_seconds":1800,"file_size_bytes":25000000,"mp3_url":"https://cdn.test.com/test.mp3","guid":"test-guid","s3_key":"podcast/2025/test.mp3"}' \
            || echo "Expected validation errors for incomplete metadata"
          echo "✅ Metadata validation script validated"

  summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [test, lint, security, validate-scripts]
    if: always()
    
    steps:
      - name: Check test results
        run: |
          echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.test.result }}" == "success" ]; then
            echo "✅ **Unit & Integration Tests**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Unit & Integration Tests**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.lint.result }}" == "success" ]; then
            echo "✅ **Code Quality**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Code Quality**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.security.result }}" == "success" ]; then
            echo "✅ **Security Checks**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Security Checks**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ "${{ needs.validate-scripts.result }}" == "success" ]; then
            echo "✅ **Script Validation**: Passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ **Script Validation**: Failed" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Test Coverage & Reports" >> $GITHUB_STEP_SUMMARY
          echo "- 📊 Test coverage report available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- 🔒 Security scan results available in artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- ⚡ Performance test results (on PR only)" >> $GITHUB_STEP_SUMMARY
          
      - name: Determine overall result
        run: |
          if [ "${{ needs.test.result }}" != "success" ] || \
             [ "${{ needs.lint.result }}" != "success" ] || \
             [ "${{ needs.security.result }}" != "success" ] || \
             [ "${{ needs.validate-scripts.result }}" != "success" ]; then
            echo "❌ One or more test jobs failed"
            exit 1
          else
            echo "✅ All tests passed successfully"
          fi